{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.8134686\n",
      "cost: 0.69315016\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "cost: 0.6931472\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Coeect:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "#x1 XOR x2 (logistic regression ) (작동 안되는게 정답이었음 ㅋㅋㅋㅋ 아나 ㅋㅋㅋㅋㅋ)\n",
    "# XOR problem\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(\"float\", shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([2,1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "#hypothesis = tf.div(1./1.+tf.exp(tf.matmul(X,W)+b))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "#True is hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(hypothesis,Y),dtype = tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n",
    "        if step%1000 == 0:\n",
    "            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n",
    "            print(\"cost:\",cost_val)\n",
    "        \n",
    "        \n",
    "        #Accuracy report\n",
    "    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \",h,\"\\nCoeect: \",p,\"\\naccuracy: \",a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.71596104\n",
      "cost: 0.6307419\n",
      "cost: 0.5129546\n",
      "cost: 0.27822495\n",
      "cost: 0.10021421\n",
      "cost: 0.053885512\n",
      "cost: 0.035932776\n",
      "cost: 0.026709894\n",
      "cost: 0.021161783\n",
      "cost: 0.017478863\n",
      "cost: 0.0148646915\n",
      "\n",
      "Hypothesis:  [[0.01737179]\n",
      " [0.98656046]\n",
      " [0.9865659 ]\n",
      " [0.01476838]] \n",
      "Prediction:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "accuracy:  100.00%\n"
     ]
    }
   ],
   "source": [
    "#x1 XOR x2 Neural Network (layer 형성하기) z1, z2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(\"float\", shape = [None, 1])\n",
    "\n",
    "#layer1이 추가된것 이외에 나머지 소스코드는 동일\n",
    "#layer의 추가로 학습가능하다는게 진짜 신기하다..\n",
    "#weight의 크기에 주의. layer1에 의해 y1,y2의  출력이 나옴\n",
    "W1 = tf.Variable(tf.random_normal([2,2]),name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = \"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "#layer1, 즉 y1,y2의 출력으로 xor을 학습시키면 xor도 학습가능\n",
    "W2 = tf.Variable(tf.random_normal([2,1]), name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]),name = \"bias2\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "#True is hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype = tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n",
    "        if step%1000 == 0:\n",
    "            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n",
    "            print(\"cost:\",cost_val)\n",
    "        \n",
    "        \n",
    "        #Accuracy report\n",
    "    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \",h,\"\\nPrediction: \",p,\"\\naccuracy:  {:.2%}\".format(a))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.7469466\n",
      "cost: 0.4647674\n",
      "cost: 0.12804735\n",
      "cost: 0.05187003\n",
      "cost: 0.029646857\n",
      "cost: 0.020029267\n",
      "cost: 0.014855608\n",
      "cost: 0.011683223\n",
      "cost: 0.009562516\n",
      "cost: 0.008055659\n",
      "cost: 0.006935424\n",
      "\n",
      "Hypothesis:  [[0.00518617]\n",
      " [0.99275726]\n",
      " [0.993079  ]\n",
      " [0.00829329]] \n",
      "Prediction:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "accuracy:  100.00%\n"
     ]
    }
   ],
   "source": [
    "#how about wide NN for XOR?\n",
    "\n",
    "#x1 XOR x2 Neural Network (layer 형성하기) z1, z2\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(\"float\", shape = [None, 1])\n",
    "\n",
    "#layer1이 10개의 출력. (wide Neural Network)\n",
    "#weight의 크기에 주의. layer1에 의해 y1,y2...의  출력이 나옴\n",
    "W1 = tf.Variable(tf.random_normal([2,10]),name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = \"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,1]), name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]),name = \"bias2\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "#True is hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype = tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n",
    "        if step%1000 == 0:\n",
    "            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n",
    "            print(\"cost:\",cost_val)\n",
    "        \n",
    "        \n",
    "        #Accuracy report\n",
    "    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \",h,\"\\nPrediction: \",p,\"\\naccuracy:  {:.2%}\".format(a))\n",
    "# -> 작은값은 더 작아지고 큰값은 더 커져서 cost가 줄어들음\n",
    "# 더 학습이 잘 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.69639146\n",
      "cost: 0.59771377\n",
      "cost: 0.05275155\n",
      "cost: 0.012338659\n",
      "cost: 0.006326462\n",
      "cost: 0.0041299406\n",
      "cost: 0.0030242365\n",
      "cost: 0.0023674509\n",
      "cost: 0.0019357011\n",
      "cost: 0.0016318567\n",
      "cost: 0.0014071119\n",
      "\n",
      "Hypothesis:  [[9.4928045e-04]\n",
      " [9.9880052e-01]\n",
      " [9.9839932e-01]\n",
      " [1.8747761e-03]] \n",
      "Prediction:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "accuracy:  100.00%\n"
     ]
    }
   ],
   "source": [
    "#how about Deep Neural Network for XOR? (넓게 말고 깊게)\n",
    "\n",
    "#x1 XOR x2 Neural Network (layer 형성하기) z1, z2\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(\"float\", shape = [None, 1])\n",
    "\n",
    "#layer1이 10개의 출력. (wide Neural Network)\n",
    "#weight의 크기에 주의. layer1에 의해 y1,y2...의  출력이 나옴\n",
    "W1 = tf.Variable(tf.random_normal([2,10]),name = \"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = \"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,10]),name = \"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([10]), name = \"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10,10]),name = \"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3)+b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10,1]), name = \"weight4\")\n",
    "b4 = tf.Variable(tf.random_normal([1]),name = \"bias4\")\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3,W4)+b4)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#Accuracy computation\n",
    "#True is hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype = tf.float32))\n",
    "\n",
    "#launch graph\n",
    "with tf.Session() as sess:\n",
    "    #Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n",
    "        if step%1000 == 0:\n",
    "            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n",
    "            print(\"cost:\",cost_val)\n",
    "        \n",
    "        \n",
    "        #Accuracy report\n",
    "    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \",h,\"\\nPrediction: \",p,\"\\naccuracy:  {:.2%}\".format(a))\n",
    "    \n",
    "# -> wide만한 학습보다 깊기까지한 학습이 더 효과가 좋음\n",
    "# 작은값은 더더 작게 큰값은 더더 크게. cost더 줄어들음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data1/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data1/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data1/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data1/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#MNIST_data 로 NN 학습시켜보기 (연습)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "tf.set_random_seed(777)\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data1/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 2.333540727\n",
      "Epoch: 0002, Cost: 2.167449287\n",
      "Epoch: 0003, Cost: 2.042444268\n",
      "Epoch: 0004, Cost: 1.901059048\n",
      "Epoch: 0005, Cost: 1.767631884\n",
      "Epoch: 0006, Cost: 1.662612824\n",
      "Epoch: 0007, Cost: 1.580704757\n",
      "Epoch: 0008, Cost: 1.517545512\n",
      "Epoch: 0009, Cost: 1.468290300\n",
      "Epoch: 0010, Cost: 1.430842588\n",
      "Epoch: 0011, Cost: 1.378834851\n",
      "Epoch: 0012, Cost: 1.248455399\n",
      "Epoch: 0013, Cost: 1.167462565\n",
      "Epoch: 0014, Cost: 1.097851815\n",
      "Epoch: 0015, Cost: 1.025703421\n",
      "Epoch: 0016, Cost: 0.975752228\n",
      "Epoch: 0017, Cost: 0.942743393\n",
      "Epoch: 0018, Cost: 0.919383645\n",
      "Epoch: 0019, Cost: 0.900535114\n",
      "Epoch: 0020, Cost: 0.885172746\n",
      "Epoch: 0021, Cost: 0.871612126\n",
      "Epoch: 0022, Cost: 0.860359461\n",
      "Epoch: 0023, Cost: 0.850082549\n",
      "Epoch: 0024, Cost: 0.841257236\n",
      "Epoch: 0025, Cost: 0.833270690\n",
      "Epoch: 0026, Cost: 0.825616351\n",
      "Epoch: 0027, Cost: 0.818567260\n",
      "Epoch: 0028, Cost: 0.811699455\n",
      "Epoch: 0029, Cost: 0.804747038\n",
      "Epoch: 0030, Cost: 0.797201076\n",
      "Epoch: 0031, Cost: 0.788215150\n",
      "Epoch: 0032, Cost: 0.777491811\n",
      "Epoch: 0033, Cost: 0.764938421\n",
      "Epoch: 0034, Cost: 0.752623512\n",
      "Epoch: 0035, Cost: 0.741455278\n",
      "Epoch: 0036, Cost: 0.730867425\n",
      "Epoch: 0037, Cost: 0.720830303\n",
      "Epoch: 0038, Cost: 0.711723710\n",
      "Epoch: 0039, Cost: 0.703487979\n",
      "Epoch: 0040, Cost: 0.696325072\n",
      "Epoch: 0041, Cost: 0.689708342\n",
      "Epoch: 0042, Cost: 0.683723946\n",
      "Epoch: 0043, Cost: 0.678559175\n",
      "Epoch: 0044, Cost: 0.674029926\n",
      "Epoch: 0045, Cost: 0.669440166\n",
      "Epoch: 0046, Cost: 0.664755968\n",
      "Epoch: 0047, Cost: 0.660333969\n",
      "Epoch: 0048, Cost: 0.656075259\n",
      "Epoch: 0049, Cost: 0.652251762\n",
      "Epoch: 0050, Cost: 0.648517862\n",
      "Epoch: 0051, Cost: 0.645179477\n",
      "Epoch: 0052, Cost: 0.642001840\n",
      "Epoch: 0053, Cost: 0.638922036\n",
      "Epoch: 0054, Cost: 0.635832448\n",
      "Epoch: 0055, Cost: 0.633196398\n",
      "Epoch: 0056, Cost: 0.630314155\n",
      "Epoch: 0057, Cost: 0.628027314\n",
      "Epoch: 0058, Cost: 0.625693365\n",
      "Epoch: 0059, Cost: 0.623380660\n",
      "Epoch: 0060, Cost: 0.621201752\n",
      "Epoch: 0061, Cost: 0.619225514\n",
      "Epoch: 0062, Cost: 0.617096034\n",
      "Epoch: 0063, Cost: 0.615076798\n",
      "Epoch: 0064, Cost: 0.613251942\n",
      "Epoch: 0065, Cost: 0.611072706\n",
      "Epoch: 0066, Cost: 0.609208757\n",
      "Epoch: 0067, Cost: 0.607349482\n",
      "Epoch: 0068, Cost: 0.605930850\n",
      "Epoch: 0069, Cost: 0.603921781\n",
      "Epoch: 0070, Cost: 0.602449735\n",
      "Epoch: 0071, Cost: 0.600823846\n",
      "Epoch: 0072, Cost: 0.599306352\n",
      "Epoch: 0073, Cost: 0.597867437\n",
      "Epoch: 0074, Cost: 0.596487447\n",
      "Epoch: 0075, Cost: 0.595069854\n",
      "Epoch: 0076, Cost: 0.593787636\n",
      "Epoch: 0077, Cost: 0.592237608\n",
      "Epoch: 0078, Cost: 0.591012719\n",
      "Epoch: 0079, Cost: 0.589823021\n",
      "Epoch: 0080, Cost: 0.588620173\n",
      "Epoch: 0081, Cost: 0.587291366\n",
      "Epoch: 0082, Cost: 0.586077542\n",
      "Epoch: 0083, Cost: 0.584950541\n",
      "Epoch: 0084, Cost: 0.584195449\n",
      "Epoch: 0085, Cost: 0.582768413\n",
      "Epoch: 0086, Cost: 0.581604286\n",
      "Epoch: 0087, Cost: 0.580864217\n",
      "Epoch: 0088, Cost: 0.579565403\n",
      "Epoch: 0089, Cost: 0.578653930\n",
      "Epoch: 0090, Cost: 0.577353045\n",
      "Epoch: 0091, Cost: 0.576554492\n",
      "Epoch: 0092, Cost: 0.575736094\n",
      "Epoch: 0093, Cost: 0.574586401\n",
      "Epoch: 0094, Cost: 0.573720262\n",
      "Epoch: 0095, Cost: 0.572894427\n",
      "Epoch: 0096, Cost: 0.571797710\n",
      "Epoch: 0097, Cost: 0.571102644\n",
      "Epoch: 0098, Cost: 0.570124951\n",
      "Epoch: 0099, Cost: 0.569140101\n",
      "Epoch: 0100, Cost: 0.568247569\n",
      "Epoch: 0101, Cost: 0.567412216\n",
      "Epoch: 0102, Cost: 0.566818816\n",
      "Epoch: 0103, Cost: 0.565762540\n",
      "Epoch: 0104, Cost: 0.564999285\n",
      "Epoch: 0105, Cost: 0.563858959\n",
      "Epoch: 0106, Cost: 0.563250081\n",
      "Epoch: 0107, Cost: 0.562392005\n",
      "Epoch: 0108, Cost: 0.561322181\n",
      "Epoch: 0109, Cost: 0.560530594\n",
      "Epoch: 0110, Cost: 0.559897301\n",
      "Epoch: 0111, Cost: 0.559082918\n",
      "Epoch: 0112, Cost: 0.558141683\n",
      "Epoch: 0113, Cost: 0.557390444\n",
      "Epoch: 0114, Cost: 0.556656957\n",
      "Epoch: 0115, Cost: 0.555588411\n",
      "Epoch: 0116, Cost: 0.554926076\n",
      "Epoch: 0117, Cost: 0.554013761\n",
      "Epoch: 0118, Cost: 0.553386413\n",
      "Epoch: 0119, Cost: 0.552403730\n",
      "Epoch: 0120, Cost: 0.551704273\n",
      "Epoch: 0121, Cost: 0.550835983\n",
      "Epoch: 0122, Cost: 0.550051681\n",
      "Epoch: 0123, Cost: 0.548925491\n",
      "Epoch: 0124, Cost: 0.548162784\n",
      "Epoch: 0125, Cost: 0.546831952\n",
      "Epoch: 0126, Cost: 0.545739893\n",
      "Epoch: 0127, Cost: 0.544220178\n",
      "Epoch: 0128, Cost: 0.542432496\n",
      "Epoch: 0129, Cost: 0.540392314\n",
      "Epoch: 0130, Cost: 0.537934437\n",
      "Epoch: 0131, Cost: 0.535349829\n",
      "Epoch: 0132, Cost: 0.531208694\n",
      "Epoch: 0133, Cost: 0.525983051\n",
      "Epoch: 0134, Cost: 0.519264805\n",
      "Epoch: 0135, Cost: 0.510966264\n",
      "Epoch: 0136, Cost: 0.501660537\n",
      "Epoch: 0137, Cost: 0.492552269\n",
      "Epoch: 0138, Cost: 0.482902166\n",
      "Epoch: 0139, Cost: 0.473676750\n",
      "Epoch: 0140, Cost: 0.465756483\n",
      "Epoch: 0141, Cost: 0.458791111\n",
      "Epoch: 0142, Cost: 0.452216603\n",
      "Epoch: 0143, Cost: 0.445637298\n",
      "Epoch: 0144, Cost: 0.440205988\n",
      "Epoch: 0145, Cost: 0.435161000\n",
      "Epoch: 0146, Cost: 0.430559825\n",
      "Epoch: 0147, Cost: 0.426534010\n",
      "Epoch: 0148, Cost: 0.422950725\n",
      "Epoch: 0149, Cost: 0.419531677\n",
      "Epoch: 0150, Cost: 0.416464799\n",
      "Epoch: 0151, Cost: 0.413949576\n",
      "Epoch: 0152, Cost: 0.411826879\n",
      "Epoch: 0153, Cost: 0.409710836\n",
      "Epoch: 0154, Cost: 0.407620686\n",
      "Epoch: 0155, Cost: 0.405286446\n",
      "Epoch: 0156, Cost: 0.403624688\n",
      "Epoch: 0157, Cost: 0.401822890\n",
      "Epoch: 0158, Cost: 0.400162408\n",
      "Epoch: 0159, Cost: 0.399057203\n",
      "Epoch: 0160, Cost: 0.397257750\n",
      "Epoch: 0161, Cost: 0.395841667\n",
      "Epoch: 0162, Cost: 0.394364794\n",
      "Epoch: 0163, Cost: 0.393015466\n",
      "Epoch: 0164, Cost: 0.391803274\n",
      "Epoch: 0165, Cost: 0.390441389\n",
      "Epoch: 0166, Cost: 0.389520066\n",
      "Epoch: 0167, Cost: 0.388131165\n",
      "Epoch: 0168, Cost: 0.387318512\n",
      "Epoch: 0169, Cost: 0.386157022\n",
      "Epoch: 0170, Cost: 0.385448268\n",
      "Epoch: 0171, Cost: 0.384420409\n",
      "Epoch: 0172, Cost: 0.383316438\n",
      "Epoch: 0173, Cost: 0.382510044\n",
      "Epoch: 0174, Cost: 0.381833737\n",
      "Epoch: 0175, Cost: 0.380800144\n",
      "Epoch: 0176, Cost: 0.379902167\n",
      "Epoch: 0177, Cost: 0.379618185\n",
      "Epoch: 0178, Cost: 0.378465898\n",
      "Epoch: 0179, Cost: 0.377492278\n",
      "Epoch: 0180, Cost: 0.376478539\n",
      "Epoch: 0181, Cost: 0.375853732\n",
      "Epoch: 0182, Cost: 0.375324684\n",
      "Epoch: 0183, Cost: 0.374509388\n",
      "Epoch: 0184, Cost: 0.373560471\n",
      "Epoch: 0185, Cost: 0.372997417\n",
      "Epoch: 0186, Cost: 0.372113669\n",
      "Epoch: 0187, Cost: 0.371420432\n",
      "Epoch: 0188, Cost: 0.370569671\n",
      "Epoch: 0189, Cost: 0.369971899\n",
      "Epoch: 0190, Cost: 0.369252048\n",
      "Epoch: 0191, Cost: 0.368316555\n",
      "Epoch: 0192, Cost: 0.367623698\n",
      "Epoch: 0193, Cost: 0.366933639\n",
      "Epoch: 0194, Cost: 0.366052600\n",
      "Epoch: 0195, Cost: 0.365529643\n",
      "Epoch: 0196, Cost: 0.364694163\n",
      "Epoch: 0197, Cost: 0.363659781\n",
      "Epoch: 0198, Cost: 0.363059229\n",
      "Epoch: 0199, Cost: 0.362285197\n",
      "Epoch: 0200, Cost: 0.361184267\n",
      "Learning finished\n",
      "Accuracy: 86.60%\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6v4Ot+JUJfEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23e901748d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADe1JREFUeJzt3WGIXPW5x/Hfc73tG9sEvZkbgo1uVqQgC93CIVxWkV6u\nLakpxL6RbjCkUJoG09JCkY1KuL5Lcr1t8YVJ2F5DszfNNoVWDCgbNBT3Vi/BUVKNtb161w1NiMmE\nFGJf9arPfTEnZY07/zPOmZkzu8/3A8vOnOecOY9jfntm5j/n/M3dBSCev6u6AQDVIPxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4L6+37ubNWqVT40NNTPXQKhzM/P69KlS9bOuqXCb2YbJD0u6TpJ\n/+Hue1LrDw0NqV6vl9klgIQsy9pet+OX/WZ2naQnJH1V0u2Sxs3s9k4fD0B/lXnPv17S2+4+5+5/\nlfQLSZu60xaAXisT/psk/WnB/bP5so8ws21mVjezeqPRKLE7AN3U80/73X3S3TN3z2q1Wq93B6BN\nZcJ/TtLaBfc/ly8DsASUCf/Lkm4zs3Vm9mlJ35B0rDttAei1jof63P19M/uupONqDvUddPc3utYZ\ngJ4qNc7v7s9KerZLvQDoI77eCwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU\n4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q\nFOEHgiL8QFClZuk1s3lJ70n6QNL77p51o6lo5ubmkvUdO3Yk60NDQy1rd911V3LbjRs3JusrVqxI\n1rF0lQp/7p/d/VIXHgdAH/GyHwiqbPhd0vNm9oqZbetGQwD6o+zL/jvd/ZyZ/aOk58zsD+4+u3CF\n/I/CNkm6+eabS+4OQLeUOvK7+7n890VJT0lav8g6k+6euXtWq9XK7A5AF3UcfjO73sw+e/W2pK9I\nOt2txgD0VpmX/aslPWVmVx/niLvPdKUrAD3XcfjdfU7SF7rYS1iPPfZYsj4zk/6bumHDhpa1AwcO\ndNRTO48tSbt27UrWx8bGSu0fvcNQHxAU4QeCIvxAUIQfCIrwA0ERfiCobpzVh5K2bNmSrBcN1z3x\nxBMta6tWrUpuu2/fvmT9hRdeSNbvuOOOZD01VHj06NHktpxO3Fsc+YGgCD8QFOEHgiL8QFCEHwiK\n8ANBEX4gKMb5l4GTJ0+2rI2Pjye33blzZ6n6lStXkvWJiYmWtdHR0eS2hw8fTtY5XbgcjvxAUIQf\nCIrwA0ERfiAowg8ERfiBoAg/EBTj/ANgZGSk1Pazs7Mta0Xj/GUVnXO/f//+lrXp6enktkXXCjhy\n5Eiy3uv/9qWOIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFU4zm9mByV9TdJFdx/Jl90o6aikIUnz\nku5z9z/3rs3lrWisfPv27cn68ePHW9aKzrev8tr4RePwqe8vSNLmzZuT9VtuuaVljWsBtHfk/5mk\na2de2CnphLvfJulEfh/AElIYfneflXT5msWbJB3Kbx+SdG+X+wLQY52+51/t7ufz2+9KWt2lfgD0\nSekP/NzdJXmrupltM7O6mdUbjUbZ3QHokk7Df8HM1khS/vtiqxXdfdLdM3fParVah7sD0G2dhv+Y\npK357a2Snu5OOwD6pTD8ZjYt6b8lfd7MzprZtyTtkfRlM3tL0t35fQBLiDXfsvdHlmVer9f7tr/l\nYm5uLlm/9dZbW9aW8jnvRd9RWLlyZbKe+n5E6joDS1mWZarX69bOunzDDwiK8ANBEX4gKMIPBEX4\ngaAIPxAUl+5eAoaHh5P11JDW1NRUcttBHuq7dOlSqe1Tp/SCIz8QFuEHgiL8QFCEHwiK8ANBEX4g\nKMIPBMU4/zKwd+/elrWi01737ElfimHnznIXZk6dlvvMM88kt33kkUeS9Q0brr2o9EeV7X2548gP\nBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzr8MpKbZ3r17d3Lbhx56KFmfnJxM1t95551kvYyiqcmX\n6+W3+4UjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVTjOb2YHJX1N0kV3H8mXPSrp25Ia+WoPu/uz\nvWoSaalz5ovG6YuUHcdPnXM/MzOT3HbLli2l9o20do78P5O02P/Bn7j7aP5D8IElpjD87j4r6XIf\negHQR2Xe83/PzF4zs4NmdkPXOgLQF52Gf7+kYUmjks5L+lGrFc1sm5nVzazeaDRarQagzzoKv7tf\ncPcP3P1DST+VtD6x7qS7Z+6e1Wq1TvsE0GUdhd/M1iy4+3VJp7vTDoB+aWeob1rSlyStMrOzkv5V\n0pfMbFSSS5qX9J0e9gigBwrD7+6LTeD+ZA96QYcmJiZa1orG6cue73/kyJFkfePGjS1rRXMKoLf4\nhh8QFOEHgiL8QFCEHwiK8ANBEX4gKC7dvQS89NJLyfqBAwda1oouf100jfWZM2eS9ampqWR9fHyx\nkeKmF198Mbnt2NhYso5yOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM8y8Bs7OzHW9b9vLXDz74\nYLJ+9913J+t79uxpWSv6jgF6iyM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOP8SUHROfS8NDw8n\n64cPH07W77///pa1Bx54ILntihUrknWUw5EfCIrwA0ERfiAowg8ERfiBoAg/EBThB4IqHOc3s7WS\npiStluSSJt39cTO7UdJRSUOS5iXd5+5/7l2rcRWdk5+6bn+vjYyMJOupKcL37duX3Jbz/XurnSP/\n+5J+6O63S/onSTvM7HZJOyWdcPfbJJ3I7wNYIgrD7+7n3f3V/PZ7kt6UdJOkTZIO5asdknRvr5oE\n0H2f6D2/mQ1J+qKkk5JWu/v5vPSumm8LACwRbYffzD4j6VeSfuDuVxbW3N3V/Dxgse22mVndzOqN\nRqNUswC6p63wm9mn1Az+z9391/niC2a2Jq+vkXRxsW3dfdLdM3fParVaN3oG0AWF4Tczk/SkpDfd\n/ccLSsckbc1vb5X0dPfbA9Ar1nzFnljB7E5J/yXpdUkf5osfVvN9/y8l3SzpjJpDfZdTj5Vlmdfr\n9bI9h3PlypVkfeXKlS1r69atS247NzfXUU/tSk0vnjrdV5JOnTqVrHPK78dlWaZ6vW7trFs4zu/u\nv5XU6sH+5ZM0BmBw8A0/ICjCDwRF+IGgCD8QFOEHgiL8QFBcunsJKBrP3r59e8ta0em+09PTyfr4\n+HiyXkbqdF9JOn36dLI+NjbWzXbC4cgPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzr8M7N27t2Xt\n+PHjyW03b96crE9NTXXU01UzMzMta6nvJ0iM4/caR34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIpx\n/mUgdb5/0bXvJyYmkvWi6wHs3r07Wd+1a1fLGuP41eLIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB\nmbunVzBbK2lK0mpJLmnS3R83s0clfVtSI1/1YXd/NvVYWZZ5vV4v3TSAxWVZpnq9bu2s286XfN6X\n9EN3f9XMPivpFTN7Lq/9xN3/vdNGAVSnMPzufl7S+fz2e2b2pqSbet0YgN76RO/5zWxI0hclncwX\nfc/MXjOzg2Z2Q4tttplZ3czqjUZjsVUAVKDt8JvZZyT9StIP3P2KpP2ShiWNqvnK4EeLbefuk+6e\nuXtWq9W60DKAbmgr/Gb2KTWD/3N3/7UkufsFd//A3T+U9FNJ63vXJoBuKwy/mZmkJyW96e4/XrB8\nzYLVvi4pPaUqgIHSzqf9d0jaIul1M7t6fujDksbNbFTN4b95Sd/pSYcAeqKdT/t/K2mxccPkmD6A\nwcY3/ICgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0EVXrq7\nqzsza0g6s2DRKkmX+tbAJzOovQ1qXxK9daqbvd3i7m1dL6+v4f/Yzs3q7p5V1kDCoPY2qH1J9Nap\nqnrjZT8QFOEHgqo6/JMV7z9lUHsb1L4keutUJb1V+p4fQHWqPvIDqEgl4TezDWb2RzN728x2VtFD\nK2Y2b2avm9kpM6t0SuF8GrSLZnZ6wbIbzew5M3sr/73oNGkV9faomZ3Ln7tTZnZPRb2tNbPfmNnv\nzewNM/t+vrzS5y7RVyXPW99f9pvZdZL+R9KXJZ2V9LKkcXf/fV8bacHM5iVl7l75mLCZ3SXpL5Km\n3H0kX/Zvki67+578D+cN7j4xIL09KukvVc/cnE8os2bhzNKS7pX0TVX43CX6uk8VPG9VHPnXS3rb\n3efc/a+SfiFpUwV9DDx3n5V0+ZrFmyQdym8fUvMfT9+16G0guPt5d381v/2epKszS1f63CX6qkQV\n4b9J0p8W3D+rwZry2yU9b2avmNm2qptZxOp82nRJelfS6iqbWUThzM39dM3M0gPz3HUy43W38YHf\nx93p7qOSvippR/7ydiB58z3bIA3XtDVzc78sMrP031T53HU643W3VRH+c5LWLrj/uXzZQHD3c/nv\ni5Ke0uDNPnzh6iSp+e+LFffzN4M0c/NiM0trAJ67QZrxuorwvyzpNjNbZ2aflvQNSccq6ONjzOz6\n/IMYmdn1kr6iwZt9+JikrfntrZKerrCXjxiUmZtbzSytip+7gZvx2t37/iPpHjU/8f9fSY9U0UOL\nvoYl/S7/eaPq3iRNq/ky8P/U/GzkW5L+QdIJSW9Jel7SjQPU239Kel3Sa2oGbU1Fvd2p5kv61ySd\nyn/uqfq5S/RVyfPGN/yAoPjADwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUP8PXytjwhGbYsAA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23e8fdbfc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 10 \n",
    " \n",
    " \n",
    " # MNIST data image of shape 28 * 28 = 784 \n",
    "X = tf.placeholder(tf.float32, [None, 784]) \n",
    " # 0 - 9 digits recognition = 10 classes \n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes]) \n",
    "\n",
    "#Using Neural Network\n",
    "output_num = 50\n",
    "W1 = tf.Variable(tf.random_normal([784,output_num])) \n",
    "b1 = tf.Variable(tf.random_normal([output_num])) \n",
    "layer1 = tf.nn.softmax(tf.matmul(X,W1) + b1)\n",
    "\n",
    "#W2 = tf.Variable(tf.random_normal([output_num, output_num])) \n",
    "#b2 = tf.Variable(tf.random_normal([output_num])) \n",
    "#layer2 = tf.nn.softmax(tf.matmul(layer1,W2) + b2)\n",
    " \n",
    "# Hypothesis (using softmax) \n",
    "W2 = tf.Variable(tf.random_normal([output_num, nb_classes])) \n",
    "b2 = tf.Variable(tf.random_normal([nb_classes])) \n",
    "hypothesis = tf.nn.softmax(tf.matmul(layer1, W2) + b2) \n",
    "\n",
    " \n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost) \n",
    "\n",
    " \n",
    "# Test model \n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1)) \n",
    " # Calculate accuracy \n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) \n",
    "\n",
    " \n",
    "# parameters \n",
    "num_epochs = 200\n",
    "batch_size = 100 \n",
    "num_iterations = int(mnist.train.num_examples / batch_size) \n",
    "\n",
    " \n",
    "with tf.Session() as sess: \n",
    "    # Initialize TensorFlow variables \n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    # Training cycle \n",
    "    for epoch in range(num_epochs): \n",
    "        avg_cost = 0 \n",
    "\n",
    " \n",
    "        for i in range(num_iterations): \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) \n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys}) \n",
    "            avg_cost += cost_val / num_iterations \n",
    "\n",
    " \n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost)) \n",
    "\n",
    " \n",
    "    print(\"Learning finished\") \n",
    "\n",
    " \n",
    "    # Test the model using test sets \n",
    "    print( \n",
    "        \"Accuracy: {:.2%}\".format( \n",
    "        accuracy.eval( \n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}) \n",
    "        ), \n",
    "    ) \n",
    "\n",
    " \n",
    "    for i in range(2):\n",
    "        # Get one and predict \n",
    "        r = random.randint(0, mnist.test.num_examples - 1) \n",
    "        print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1))) \n",
    "        print( \n",
    "            \"Prediction: \", \n",
    "            sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}), \n",
    "        ) \n",
    "\n",
    " \n",
    "        plt.imshow( \n",
    "            mnist.test.images[r : r + 1].reshape(28, 28), \n",
    "            cmap=\"Greys\", \n",
    "            interpolation=\"nearest\", \n",
    "        ) \n",
    "        plt.show() \n",
    "#deep한 layer를 사용하면 오히려 cost감소가 너무 더뎌지는것이 관측됨.\n",
    "#one_layer에서 num_epochs를 늘려야 겨우 비슷하게나마 학습되는것이 관측.\n",
    "#근데 아무리해도 cost값은 내려가도 accuracy값이 만족스럽지 않음... 아마도 다음 수업에서 이유 알려줄듯?\n",
    "#앗! 그런데 layer 한층에 output_num = 55, num_epochs = 100 으로 학습해서 91.39%의 accuracy 얻음!! 오예!!\n",
    "#91.39%면 꽤 높은 정확도인듯 나치고는."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7168676\n",
      "100 0.6908567\n",
      "200 0.49870038\n",
      "300 0.2253619\n",
      "400 0.12291672\n",
      "500 0.07914585\n",
      "600 0.056153916\n",
      "700 0.042352118\n",
      "800 0.033301707\n",
      "900 0.026986767\n",
      "1000 0.022373727\n",
      "1100 0.018883027\n",
      "1200 0.016166918\n",
      "1300 0.014005077\n",
      "1400 0.012251835\n",
      "1500 0.010807454\n",
      "1600 0.009601374\n",
      "1700 0.008582564\n",
      "1800 0.0077132024\n",
      "1900 0.006964792\n",
      "2000 0.0063154213\n",
      "2100 0.0057480196\n",
      "2200 0.0052491734\n",
      "2300 0.0048080757\n",
      "2400 0.0044160266\n",
      "2500 0.0040659807\n",
      "2600 0.003752165\n",
      "2700 0.0034696562\n",
      "2800 0.003214465\n",
      "2900 0.0029831931\n",
      "3000 0.0027729843\n",
      "3100 0.0025813607\n",
      "3200 0.0024062647\n",
      "3300 0.0022458504\n",
      "3400 0.0020985582\n",
      "3500 0.0019630534\n",
      "3600 0.0018381808\n",
      "3700 0.0017228327\n",
      "3800 0.0016161547\n",
      "3900 0.0015173084\n",
      "4000 0.0014256047\n",
      "4100 0.0013404004\n",
      "4200 0.0012611565\n",
      "4300 0.0011873196\n",
      "4400 0.0011184708\n",
      "4500 0.0010541915\n",
      "4600 0.0009941373\n",
      "4700 0.0009379648\n",
      "4800 0.00088534516\n",
      "4900 0.00083605415\n",
      "5000 0.00078983756\n",
      "5100 0.0007464565\n",
      "5200 0.00070570176\n",
      "5300 0.00066743896\n",
      "5400 0.000631429\n",
      "5500 0.00059755263\n",
      "5600 0.00056566036\n",
      "5700 0.000535588\n",
      "5800 0.0005072609\n",
      "5900 0.00048054475\n",
      "6000 0.00045537972\n",
      "6100 0.000431557\n",
      "6200 0.00040910635\n",
      "6300 0.00038787856\n",
      "6400 0.00036784378\n",
      "6500 0.00034889753\n",
      "6600 0.00033101003\n",
      "6700 0.0003140619\n",
      "6800 0.00029800838\n",
      "6900 0.00028281967\n",
      "7000 0.00026846584\n",
      "7100 0.00025488733\n",
      "7200 0.00024196476\n",
      "7300 0.00022975782\n",
      "7400 0.00021822171\n",
      "7500 0.0002072223\n",
      "7600 0.00019683407\n",
      "7700 0.00018698249\n",
      "7800 0.00017763776\n",
      "7900 0.00016878491\n",
      "8000 0.00016036436\n",
      "8100 0.00015236119\n",
      "8200 0.00014483496\n",
      "8300 0.00013763667\n",
      "8400 0.0001307961\n",
      "8500 0.00012434309\n",
      "8600 0.000118203054\n",
      "8700 0.00011237602\n",
      "8800 0.00010678747\n",
      "8900 0.00010154171\n",
      "9000 9.6549324e-05\n",
      "9100 9.17805e-05\n",
      "9200 8.7265034e-05\n",
      "9300 8.2973136e-05\n",
      "9400 7.8919686e-05\n",
      "9500 7.5045085e-05\n",
      "9600 7.134933e-05\n",
      "9700 6.786221e-05\n",
      "9800 6.4539025e-05\n",
      "9900 6.136488e-05\n",
      "10000 5.8384467e-05\n",
      "\n",
      "Hypothesis:\n",
      "[[6.1310318e-05]\n",
      " [9.9993694e-01]\n",
      " [9.9995077e-01]\n",
      " [5.9751477e-05]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Lab 9 XOR\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight_1\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name=\"bias_1\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight_2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias_2\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, summary, cost_val = sess.run(\n",
    "            [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
