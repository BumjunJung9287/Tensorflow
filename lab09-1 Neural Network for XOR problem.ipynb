{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab09-1 Neural Network for XOR problem.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"tee8vjUPnGXk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":485},"outputId":"f01235a9-a4a9-4fb8-d07c-36f38d4bf7a3","executionInfo":{"status":"ok","timestamp":1554285173004,"user_tz":-540,"elapsed":7331,"user":{"displayName":"정범준","photoUrl":"https://lh5.googleusercontent.com/-27iY8j7CaS0/AAAAAAAAAAI/AAAAAAAAAC4/5DN-E5OL3Eo/s64/photo.jpg","userId":"01726864658215807628"}}},"cell_type":"code","source":["#x1 XOR x2 (logistic regression ) (작동 안되는게 정답이었음 ㅋㅋㅋㅋ 아나 ㅋㅋㅋㅋㅋ)\n","# XOR problem\n","import tensorflow as tf\n","import numpy as np\n","\n","x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n","y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n","\n","X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n","Y = tf.placeholder(\"float\", shape = [None, 1])\n","W = tf.Variable(tf.random_normal([2,1]), name = \"weight\")\n","b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n","\n","#hypothesis = tf.div(1./1.+tf.exp(tf.matmul(X,W)+b))\n","hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n","\n","cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n","\n","#Accuracy computation\n","#True is hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(hypothesis,Y),dtype = tf.float32))\n","\n","#launch graph\n","with tf.Session() as sess:\n","    #Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","    \n","    for step in range(10001):\n","        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n","        if step%1000 == 0:\n","            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n","            print(\"cost:\",cost_val)\n","        \n","        \n","        #Accuracy report\n","    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n","    print(\"\\nHypothesis: \",h,\"\\nCoeect: \",p,\"\\naccuracy: \",a)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","cost: 0.92503524\n","cost: 0.69314766\n","cost: 0.6931472\n","cost: 0.6931472\n","cost: 0.6931472\n","cost: 0.6931472\n","cost: 0.6931472\n","cost: 0.6931472\n","cost: 0.6931472\n","cost: 0.6931472\n","cost: 0.6931472\n","\n","Hypothesis:  [[0.5]\n"," [0.5]\n"," [0.5]\n"," [0.5]] \n","Coeect:  [[0.]\n"," [0.]\n"," [0.]\n"," [0.]] \n","accuracy:  0.0\n"],"name":"stdout"}]},{"metadata":{"id":"p4BbUO-mnWFM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":409},"outputId":"d3203e05-1ad6-4797-e6c9-a54968029ed6","executionInfo":{"status":"ok","timestamp":1554285386362,"user_tz":-540,"elapsed":13675,"user":{"displayName":"정범준","photoUrl":"https://lh5.googleusercontent.com/-27iY8j7CaS0/AAAAAAAAAAI/AAAAAAAAAC4/5DN-E5OL3Eo/s64/photo.jpg","userId":"01726864658215807628"}}},"cell_type":"code","source":["#x1 XOR x2 Neural Network (layer 형성하기) z1, z2\n","import tensorflow as tf\n","import numpy as np\n","\n","x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n","y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n","\n","X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n","Y = tf.placeholder(\"float\", shape = [None, 1])\n","\n","#layer1이 추가된것 이외에 나머지 소스코드는 동일\n","#layer의 추가로 학습가능하다는게 진짜 신기하다..\n","#weight의 크기에 주의. layer1에 의해 y1,y2의  출력이 나옴\n","W1 = tf.Variable(tf.random_normal([2,2]),name = \"weight1\")\n","b1 = tf.Variable(tf.random_normal([2]), name = \"bias1\")\n","layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n","\n","#layer1, 즉 y1,y2의 출력으로 xor을 학습시키면 xor도 학습가능\n","W2 = tf.Variable(tf.random_normal([2,1]), name = \"weight2\")\n","b2 = tf.Variable(tf.random_normal([1]),name = \"bias2\")\n","hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n","\n","\n","cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n","\n","#Accuracy computation\n","#True is hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype = tf.float32))\n","\n","#launch graph\n","with tf.Session() as sess:\n","    #Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","    \n","    for step in range(10001):\n","        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n","        if step%1000 == 0:\n","            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n","            print(\"cost:\",cost_val)\n","        \n","        \n","        #Accuracy report\n","    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n","    print(\"\\nHypothesis: \",h,\"\\nPrediction: \",p,\"\\naccuracy:  {:.2%}\".format(a))\n","    "],"execution_count":2,"outputs":[{"output_type":"stream","text":["cost: 0.8663662\n","cost: 0.6932412\n","cost: 0.6923208\n","cost: 0.6897037\n","cost: 0.6710062\n","cost: 0.58073497\n","cost: 0.35859028\n","cost: 0.15776327\n","cost: 0.09129598\n","cost: 0.06264144\n","cost: 0.04722185\n","\n","Hypothesis:  [[0.04177612]\n"," [0.9478278 ]\n"," [0.9458164 ]\n"," [0.03625092]] \n","Prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  100.00%\n"],"name":"stdout"}]},{"metadata":{"id":"c6cWMfRWnaKL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":409},"outputId":"9a932af6-c5a8-4b4f-ccef-68653b1a9572","executionInfo":{"status":"ok","timestamp":1554285452592,"user_tz":-540,"elapsed":13295,"user":{"displayName":"정범준","photoUrl":"https://lh5.googleusercontent.com/-27iY8j7CaS0/AAAAAAAAAAI/AAAAAAAAAC4/5DN-E5OL3Eo/s64/photo.jpg","userId":"01726864658215807628"}}},"cell_type":"code","source":["#how about wide NN for XOR?\n","\n","#x1 XOR x2 Neural Network (layer 형성하기) z1, z2\n","\n","x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n","y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n","\n","X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n","Y = tf.placeholder(\"float\", shape = [None, 1])\n","\n","#layer1이 10개의 출력. (wide Neural Network)\n","#weight의 크기에 주의. layer1에 의해 y1,y2...의  출력이 나옴\n","W1 = tf.Variable(tf.random_normal([2,10]),name = \"weight1\")\n","b1 = tf.Variable(tf.random_normal([10]), name = \"bias1\")\n","layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n","\n","W2 = tf.Variable(tf.random_normal([10,1]), name = \"weight2\")\n","b2 = tf.Variable(tf.random_normal([1]),name = \"bias2\")\n","hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n","\n","\n","cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n","\n","#Accuracy computation\n","#True is hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype = tf.float32))\n","\n","#launch graph\n","with tf.Session() as sess:\n","    #Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","    \n","    for step in range(10001):\n","        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n","        if step%1000 == 0:\n","            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n","            print(\"cost:\",cost_val)\n","        \n","        \n","        #Accuracy report\n","    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n","    print(\"\\nHypothesis: \",h,\"\\nPrediction: \",p,\"\\naccuracy:  {:.2%}\".format(a))\n","# -> 작은값은 더 작아지고 큰값은 더 커져서 cost가 줄어들음\n","# 더 학습이 잘 됨."],"execution_count":3,"outputs":[{"output_type":"stream","text":["cost: 0.78604996\n","cost: 0.59444976\n","cost: 0.21718867\n","cost: 0.06514079\n","cost: 0.03188567\n","cost: 0.019967852\n","cost: 0.014191889\n","cost: 0.010869353\n","cost: 0.008740524\n","cost: 0.0072725257\n","cost: 0.006205007\n","\n","Hypothesis:  [[0.00578073]\n"," [0.9933375 ]\n"," [0.9942266 ]\n"," [0.00652623]] \n","Prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  100.00%\n"],"name":"stdout"}]},{"metadata":{"id":"eer7AJNzoakA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":409},"outputId":"476aa53d-3171-4fe6-d4c4-bf3879441b7c","executionInfo":{"status":"ok","timestamp":1554285473182,"user_tz":-540,"elapsed":16796,"user":{"displayName":"정범준","photoUrl":"https://lh5.googleusercontent.com/-27iY8j7CaS0/AAAAAAAAAAI/AAAAAAAAAC4/5DN-E5OL3Eo/s64/photo.jpg","userId":"01726864658215807628"}}},"cell_type":"code","source":["#how about Deep Neural Network for XOR? (넓게 말고 깊게)\n","\n","#x1 XOR x2 Neural Network (layer 형성하기) z1, z2\n","\n","x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n","y_data = np.array([[0],  [1],  [1],  [0]],dtype = np.float32)\n","\n","X = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n","Y = tf.placeholder(\"float\", shape = [None, 1])\n","\n","#layer1이 10개의 출력. (wide Neural Network)\n","#weight의 크기에 주의. layer1에 의해 y1,y2...의  출력이 나옴\n","W1 = tf.Variable(tf.random_normal([2,10]),name = \"weight1\")\n","b1 = tf.Variable(tf.random_normal([10]), name = \"bias1\")\n","layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n","\n","W2 = tf.Variable(tf.random_normal([10,10]),name = \"weight2\")\n","b2 = tf.Variable(tf.random_normal([10]), name = \"bias2\")\n","layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n","\n","W3 = tf.Variable(tf.random_normal([10,10]),name = \"weight3\")\n","b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n","layer3 = tf.sigmoid(tf.matmul(layer2,W3)+b3)\n","\n","W4 = tf.Variable(tf.random_normal([10,1]), name = \"weight4\")\n","b4 = tf.Variable(tf.random_normal([1]),name = \"bias4\")\n","hypothesis = tf.sigmoid(tf.matmul(layer3,W4)+b4)\n","\n","\n","cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n","\n","#Accuracy computation\n","#True is hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype = tf.float32))\n","\n","#launch graph\n","with tf.Session() as sess:\n","    #Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","    \n","    for step in range(10001):\n","        sess.run(train, feed_dict = {X:x_data,Y:y_data})\n","        if step%1000 == 0:\n","            cost_val = sess.run(cost, feed_dict = {X:x_data, Y:y_data})# cost.eval(session = sess, feed_dict = {X:x_data, Y:y_data})\n","            print(\"cost:\",cost_val)\n","        \n","        \n","        #Accuracy report\n","    h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X:x_data, Y:y_data})\n","    print(\"\\nHypothesis: \",h,\"\\nPrediction: \",p,\"\\naccuracy:  {:.2%}\".format(a))\n","    \n","# -> wide만한 학습보다 깊기까지한 학습이 더 효과가 좋음\n","# 작은값은 더더 작게 큰값은 더더 크게. cost더 줄어들음"],"execution_count":4,"outputs":[{"output_type":"stream","text":["cost: 1.0131955\n","cost: 0.16869633\n","cost: 0.018434986\n","cost: 0.00835059\n","cost: 0.005222288\n","cost: 0.0037465189\n","cost: 0.0028991639\n","cost: 0.0023534787\n","cost: 0.0019744928\n","cost: 0.0016968247\n","cost: 0.0014851815\n","\n","Hypothesis:  [[0.00120515]\n"," [0.998242  ]\n"," [0.9987    ]\n"," [0.00167307]] \n","Prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  100.00%\n"],"name":"stdout"}]},{"metadata":{"id":"F8YqFpECoeu2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"outputId":"524ae07a-83b3-4198-a466-fe1a2d1e1e1f","executionInfo":{"status":"ok","timestamp":1554285486398,"user_tz":-540,"elapsed":1149,"user":{"displayName":"정범준","photoUrl":"https://lh5.googleusercontent.com/-27iY8j7CaS0/AAAAAAAAAAI/AAAAAAAAAC4/5DN-E5OL3Eo/s64/photo.jpg","userId":"01726864658215807628"}}},"cell_type":"code","source":["#MNIST_data 로 NN 학습시켜보기 (연습)\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","import random\n","tf.set_random_seed(777)\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"MNIST_data1/\", one_hot=True)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Extracting MNIST_data1/train-images-idx3-ubyte.gz\n","Extracting MNIST_data1/train-labels-idx1-ubyte.gz\n","Extracting MNIST_data1/t10k-images-idx3-ubyte.gz\n","Extracting MNIST_data1/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"metadata":{"id":"rP1pkAFSokg_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4599},"outputId":"7331c2e5-8cf6-4e60-c040-1f28835e43fb","executionInfo":{"status":"ok","timestamp":1554285729511,"user_tz":-540,"elapsed":221559,"user":{"displayName":"정범준","photoUrl":"https://lh5.googleusercontent.com/-27iY8j7CaS0/AAAAAAAAAAI/AAAAAAAAAC4/5DN-E5OL3Eo/s64/photo.jpg","userId":"01726864658215807628"}}},"cell_type":"code","source":["nb_classes = 10 \n"," \n"," \n"," # MNIST data image of shape 28 * 28 = 784 \n","X = tf.placeholder(tf.float32, [None, 784]) \n"," # 0 - 9 digits recognition = 10 classes \n","Y = tf.placeholder(tf.float32, [None, nb_classes]) \n","\n","#Using Neural Network\n","output_num = 50\n","W1 = tf.Variable(tf.random_normal([784,output_num])) \n","b1 = tf.Variable(tf.random_normal([output_num])) \n","layer1 = tf.nn.softmax(tf.matmul(X,W1) + b1)\n","\n","#W2 = tf.Variable(tf.random_normal([output_num, output_num])) \n","#b2 = tf.Variable(tf.random_normal([output_num])) \n","#layer2 = tf.nn.softmax(tf.matmul(layer1,W2) + b2)\n"," \n","# Hypothesis (using softmax) \n","W2 = tf.Variable(tf.random_normal([output_num, nb_classes])) \n","b2 = tf.Variable(tf.random_normal([nb_classes])) \n","hypothesis = tf.nn.softmax(tf.matmul(layer1, W2) + b2) \n","\n"," \n","cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) \n","train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost) \n","\n"," \n","# Test model \n","is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1)) \n"," # Calculate accuracy \n","accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) \n","\n"," \n","# parameters \n","num_epochs = 200\n","batch_size = 100 \n","num_iterations = int(mnist.train.num_examples / batch_size) \n","\n"," \n","with tf.Session() as sess: \n","    # Initialize TensorFlow variables \n","    sess.run(tf.global_variables_initializer()) \n","    # Training cycle \n","    for epoch in range(num_epochs): \n","        avg_cost = 0 \n","\n"," \n","        for i in range(num_iterations): \n","            batch_xs, batch_ys = mnist.train.next_batch(batch_size) \n","            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys}) \n","            avg_cost += cost_val / num_iterations \n","\n"," \n","        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost)) \n","\n"," \n","    print(\"Learning finished\") \n","\n"," \n","    # Test the model using test sets \n","    print( \n","        \"Accuracy: {:.2%}\".format( \n","        accuracy.eval( \n","            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}) \n","        ), \n","    ) \n","\n"," \n","    for i in range(2):\n","        # Get one and predict \n","        r = random.randint(0, mnist.test.num_examples - 1) \n","        print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1))) \n","        print( \n","            \"Prediction: \", \n","            sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}), \n","        ) \n","\n"," \n","        plt.imshow( \n","            mnist.test.images[r : r + 1].reshape(28, 28), \n","            cmap=\"Greys\", \n","            interpolation=\"nearest\", \n","        ) \n","        plt.show() \n","#deep한 layer를 사용하면 오히려 cost감소가 너무 더뎌지는것이 관측됨.\n","#one_layer에서 num_epochs를 늘려야 겨우 비슷하게나마 학습되는것이 관측.\n","#근데 아무리해도 cost값은 내려가도 accuracy값이 만족스럽지 않음... 아마도 다음 수업에서 이유 알려줄듯?\n","#앗! 그런데 layer 한층에 output_num = 55, num_epochs = 100 으로 학습해서 91.39%의 accuracy 얻음!! 오예!!\n","#91.39%면 꽤 높은 정확도인듯 나치고는."],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch: 0001, Cost: 2.227771677\n","Epoch: 0002, Cost: 1.790985774\n","Epoch: 0003, Cost: 1.635107522\n","Epoch: 0004, Cost: 1.538924884\n","Epoch: 0005, Cost: 1.453332590\n","Epoch: 0006, Cost: 1.381762269\n","Epoch: 0007, Cost: 1.320704583\n","Epoch: 0008, Cost: 1.272027738\n","Epoch: 0009, Cost: 1.235802109\n","Epoch: 0010, Cost: 1.207248215\n","Epoch: 0011, Cost: 1.182362826\n","Epoch: 0012, Cost: 1.162739255\n","Epoch: 0013, Cost: 1.145295643\n","Epoch: 0014, Cost: 1.128017817\n","Epoch: 0015, Cost: 1.105646763\n","Epoch: 0016, Cost: 1.065359680\n","Epoch: 0017, Cost: 1.027465341\n","Epoch: 0018, Cost: 1.001246840\n","Epoch: 0019, Cost: 0.981511811\n","Epoch: 0020, Cost: 0.965764829\n","Epoch: 0021, Cost: 0.952753433\n","Epoch: 0022, Cost: 0.942429821\n","Epoch: 0023, Cost: 0.933846764\n","Epoch: 0024, Cost: 0.925842912\n","Epoch: 0025, Cost: 0.919915050\n","Epoch: 0026, Cost: 0.914371379\n","Epoch: 0027, Cost: 0.909335029\n","Epoch: 0028, Cost: 0.904683248\n","Epoch: 0029, Cost: 0.900203911\n","Epoch: 0030, Cost: 0.896312731\n","Epoch: 0031, Cost: 0.892588266\n","Epoch: 0032, Cost: 0.888613588\n","Epoch: 0033, Cost: 0.885031230\n","Epoch: 0034, Cost: 0.881213845\n","Epoch: 0035, Cost: 0.878095209\n","Epoch: 0036, Cost: 0.874475112\n","Epoch: 0037, Cost: 0.871197585\n","Epoch: 0038, Cost: 0.868395683\n","Epoch: 0039, Cost: 0.865220586\n","Epoch: 0040, Cost: 0.862262293\n","Epoch: 0041, Cost: 0.859442092\n","Epoch: 0042, Cost: 0.856485065\n","Epoch: 0043, Cost: 0.853718874\n","Epoch: 0044, Cost: 0.851170441\n","Epoch: 0045, Cost: 0.848520005\n","Epoch: 0046, Cost: 0.845830847\n","Epoch: 0047, Cost: 0.843381021\n","Epoch: 0048, Cost: 0.840789191\n","Epoch: 0049, Cost: 0.838287278\n","Epoch: 0050, Cost: 0.835899765\n","Epoch: 0051, Cost: 0.833614259\n","Epoch: 0052, Cost: 0.831358104\n","Epoch: 0053, Cost: 0.828952598\n","Epoch: 0054, Cost: 0.826555973\n","Epoch: 0055, Cost: 0.823870306\n","Epoch: 0056, Cost: 0.821116374\n","Epoch: 0057, Cost: 0.818519795\n","Epoch: 0058, Cost: 0.815800691\n","Epoch: 0059, Cost: 0.813082848\n","Epoch: 0060, Cost: 0.810623067\n","Epoch: 0061, Cost: 0.807884553\n","Epoch: 0062, Cost: 0.805061008\n","Epoch: 0063, Cost: 0.802713550\n","Epoch: 0064, Cost: 0.800694772\n","Epoch: 0065, Cost: 0.798411670\n","Epoch: 0066, Cost: 0.795688199\n","Epoch: 0067, Cost: 0.793138138\n","Epoch: 0068, Cost: 0.789624992\n","Epoch: 0069, Cost: 0.785793760\n","Epoch: 0070, Cost: 0.781079726\n","Epoch: 0071, Cost: 0.775071039\n","Epoch: 0072, Cost: 0.767939633\n","Epoch: 0073, Cost: 0.760683274\n","Epoch: 0074, Cost: 0.754604285\n","Epoch: 0075, Cost: 0.749371804\n","Epoch: 0076, Cost: 0.745229179\n","Epoch: 0077, Cost: 0.741159593\n","Epoch: 0078, Cost: 0.737784976\n","Epoch: 0079, Cost: 0.734401049\n","Epoch: 0080, Cost: 0.731251211\n","Epoch: 0081, Cost: 0.728126795\n","Epoch: 0082, Cost: 0.725240477\n","Epoch: 0083, Cost: 0.722656231\n","Epoch: 0084, Cost: 0.719882489\n","Epoch: 0085, Cost: 0.717913989\n","Epoch: 0086, Cost: 0.715657036\n","Epoch: 0087, Cost: 0.713440953\n","Epoch: 0088, Cost: 0.711430200\n","Epoch: 0089, Cost: 0.709619294\n","Epoch: 0090, Cost: 0.707768289\n","Epoch: 0091, Cost: 0.706236199\n","Epoch: 0092, Cost: 0.704315491\n","Epoch: 0093, Cost: 0.702651047\n","Epoch: 0094, Cost: 0.700828014\n","Epoch: 0095, Cost: 0.698608072\n","Epoch: 0096, Cost: 0.696580907\n","Epoch: 0097, Cost: 0.694375149\n","Epoch: 0098, Cost: 0.691885646\n","Epoch: 0099, Cost: 0.689075267\n","Epoch: 0100, Cost: 0.685550348\n","Epoch: 0101, Cost: 0.681657144\n","Epoch: 0102, Cost: 0.677420768\n","Epoch: 0103, Cost: 0.672037639\n","Epoch: 0104, Cost: 0.665038123\n","Epoch: 0105, Cost: 0.654330387\n","Epoch: 0106, Cost: 0.640448542\n","Epoch: 0107, Cost: 0.624502488\n","Epoch: 0108, Cost: 0.611118806\n","Epoch: 0109, Cost: 0.600915847\n","Epoch: 0110, Cost: 0.592255338\n","Epoch: 0111, Cost: 0.585039628\n","Epoch: 0112, Cost: 0.577941601\n","Epoch: 0113, Cost: 0.571341399\n","Epoch: 0114, Cost: 0.564791376\n","Epoch: 0115, Cost: 0.558767871\n","Epoch: 0116, Cost: 0.551530156\n","Epoch: 0117, Cost: 0.539556878\n","Epoch: 0118, Cost: 0.518819159\n","Epoch: 0119, Cost: 0.495835936\n","Epoch: 0120, Cost: 0.475024793\n","Epoch: 0121, Cost: 0.454956965\n","Epoch: 0122, Cost: 0.440143975\n","Epoch: 0123, Cost: 0.428753663\n","Epoch: 0124, Cost: 0.418856419\n","Epoch: 0125, Cost: 0.409091264\n","Epoch: 0126, Cost: 0.399683356\n","Epoch: 0127, Cost: 0.391699108\n","Epoch: 0128, Cost: 0.383281253\n","Epoch: 0129, Cost: 0.375257818\n","Epoch: 0130, Cost: 0.368128649\n","Epoch: 0131, Cost: 0.361190123\n","Epoch: 0132, Cost: 0.354842425\n","Epoch: 0133, Cost: 0.349089757\n","Epoch: 0134, Cost: 0.344584439\n","Epoch: 0135, Cost: 0.339951358\n","Epoch: 0136, Cost: 0.335888425\n","Epoch: 0137, Cost: 0.332153096\n","Epoch: 0138, Cost: 0.329038731\n","Epoch: 0139, Cost: 0.325676354\n","Epoch: 0140, Cost: 0.323004994\n","Epoch: 0141, Cost: 0.319862864\n","Epoch: 0142, Cost: 0.317524756\n","Epoch: 0143, Cost: 0.314953485\n","Epoch: 0144, Cost: 0.312545604\n","Epoch: 0145, Cost: 0.310360599\n","Epoch: 0146, Cost: 0.308006840\n","Epoch: 0147, Cost: 0.305794626\n","Epoch: 0148, Cost: 0.303837156\n","Epoch: 0149, Cost: 0.301986712\n","Epoch: 0150, Cost: 0.300022121\n","Epoch: 0151, Cost: 0.298259291\n","Epoch: 0152, Cost: 0.296491826\n","Epoch: 0153, Cost: 0.294514176\n","Epoch: 0154, Cost: 0.292727939\n","Epoch: 0155, Cost: 0.291241751\n","Epoch: 0156, Cost: 0.289777787\n","Epoch: 0157, Cost: 0.288023352\n","Epoch: 0158, Cost: 0.286521028\n","Epoch: 0159, Cost: 0.285179970\n","Epoch: 0160, Cost: 0.283976795\n","Epoch: 0161, Cost: 0.282166433\n","Epoch: 0162, Cost: 0.281308260\n","Epoch: 0163, Cost: 0.279768925\n","Epoch: 0164, Cost: 0.278582079\n","Epoch: 0165, Cost: 0.277316477\n","Epoch: 0166, Cost: 0.276255701\n","Epoch: 0167, Cost: 0.275069705\n","Epoch: 0168, Cost: 0.273958059\n","Epoch: 0169, Cost: 0.272710981\n","Epoch: 0170, Cost: 0.271577045\n","Epoch: 0171, Cost: 0.270630150\n","Epoch: 0172, Cost: 0.269542250\n","Epoch: 0173, Cost: 0.268581338\n","Epoch: 0174, Cost: 0.267429884\n","Epoch: 0175, Cost: 0.266489855\n","Epoch: 0176, Cost: 0.265508865\n","Epoch: 0177, Cost: 0.264585317\n","Epoch: 0178, Cost: 0.263497640\n","Epoch: 0179, Cost: 0.262466249\n","Epoch: 0180, Cost: 0.261759230\n","Epoch: 0181, Cost: 0.260572975\n","Epoch: 0182, Cost: 0.259856318\n","Epoch: 0183, Cost: 0.259054369\n","Epoch: 0184, Cost: 0.258257740\n","Epoch: 0185, Cost: 0.256976911\n","Epoch: 0186, Cost: 0.256376181\n","Epoch: 0187, Cost: 0.255583006\n","Epoch: 0188, Cost: 0.254668935\n","Epoch: 0189, Cost: 0.253814889\n","Epoch: 0190, Cost: 0.253068903\n","Epoch: 0191, Cost: 0.252227325\n","Epoch: 0192, Cost: 0.251341249\n","Epoch: 0193, Cost: 0.250555207\n","Epoch: 0194, Cost: 0.249812171\n","Epoch: 0195, Cost: 0.249061279\n","Epoch: 0196, Cost: 0.248171571\n","Epoch: 0197, Cost: 0.247650083\n","Epoch: 0198, Cost: 0.246697279\n","Epoch: 0199, Cost: 0.246335524\n","Epoch: 0200, Cost: 0.245521606\n","Learning finished\n","Accuracy: 92.70%\n","Label:  [8]\n","Prediction:  [8]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIyOCUQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 576x396 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Label:  [3]\n","Prediction:  [3]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYbI0NAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 576x396 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"OdX7lX9VorUc","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}